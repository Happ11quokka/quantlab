from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import requests
from datetime import datetime, timedelta
import json
import os


def get_full_text(article_url: str) -> str:
    options = Options()
    options.add_argument('--headless')
    options.add_argument('--disable-gpu')
    driver = webdriver.Chrome(options=options)
    driver.get(article_url)

    # í˜ì´ì§€ ì „ì²´ê°€ ë¡œë”©ë  ë•Œê¹Œì§€ ëŒ€ê¸° (í•„ìš”ì‹œ WebDriverWait ì‚¬ìš© ê°€ëŠ¥)
    time.sleep(2)

    # í˜ì´ì§€ì˜ <body> ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    full_text = driver.find_element(By.TAG_NAME, "body").text
    driver.quit()
    return full_text


def get_article_content(article_url: str) -> str:
    """
    Seleniumì„ ì‚¬ìš©í•´ ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ë³¸ë¬¸ì„ ê°€ì ¸ì˜¨ë‹¤.
    """
    try:
        content = get_full_text(article_url)
    except Exception as e:
        print(f"âŒ Error retrieving full text for {article_url}: {e}")
        content = ""
    return content


def crawl_nate_news(start_date="20250101", end_date="20250102"):
    """
    ë‚ ì§œ ë²”ìœ„ë¥¼ ì§€ì •í•˜ë©´, í•´ë‹¹ ë‚ ì§œì˜ ê¸°ì‚¬ ëª©ë¡ í˜ì´ì§€ì—ì„œ
    ê¸°ì‚¬ URLì„ ìˆ˜ì§‘í•˜ê³  get_article_content()ë¥¼ í†µí•´ ë³¸ë¬¸ì„ ê°€ì ¸ì™€ ê²°ê³¼ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    base_url = "https://news.nate.com/MediaList?cp=jo&cate=eco&mid=n1101&type=c&date="
    current_date = datetime.strptime(start_date, "%Y%m%d")
    end_date = datetime.strptime(end_date, "%Y%m%d")

    all_articles = []

    while current_date <= end_date:
        date_str = current_date.strftime("%Y%m%d")
        print(f"ğŸ“° í¬ë¡¤ë§ ì¤‘: {date_str}")
        url = base_url + date_str

        # ê¸°ì‚¬ ëª©ë¡ í˜ì´ì§€ ìš”ì²­
        res = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
        soup = BeautifulSoup(res.content, "html.parser")

        # ê¸°ì‚¬ ëª©ë¡ì—ì„œ a.lt1 íƒœê·¸(ê¸°ì‚¬ ë§í¬) ì¶”ì¶œ
        links = soup.select("a.lt1")
        for link in links:
            title = link.get_text(strip=True)
            article_url = link.get("href")
            if article_url:
                if not article_url.startswith("http"):
                    article_url = "https:" + article_url

                print(f"  â¤ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘: \"{title}\" | URL: {article_url}")
                content = get_article_content(article_url)
                if not content:
                    print(f"   âŒ ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {article_url}")

                all_articles.append({
                    "date": date_str,
                    "title": title,
                    "url": article_url,
                    "content": content
                })

                time.sleep(0.5)  # ê³¼ë„í•œ ìš”ì²­ ë°©ì§€

        current_date += timedelta(days=1)

    return all_articles


def save_to_json(data, filename="nate_news.json"):
    os.makedirs("results", exist_ok=True)
    filepath = os.path.join("results", filename)
    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)
    print(f"âœ… JSON ì €ì¥ ì™„ë£Œ: {filepath}")


if __name__ == "__main__":
    articles = crawl_nate_news()
    save_to_json(articles)
